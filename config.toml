[llm]
#    model = "gpt-4-turbo"  # Example for OpenAI
#    api_key = "" # Example for OpenAI (use env var preferably)
#    base_url = "https://api.openai.com/v1/chat/completions" # Example for OpenAI
    model = "gemini-1.5-flash-latest" # Example for Gemini
    api_key = "" # Example for Gemini (use env var preferably)
    base_url = "https://generativelanguage.googleapis.com/v1beta/models" # Example for Gemini
    parameters = {}
    max_tokens = 2000
    [llm.ocr]
    mistral_api_key = ""

[llm.triple_extraction]
#    model = "gpt-4-turbo"  # Example for OpenAI
#    api_key = "" # Example for OpenAI (use env var preferably)
#    base_url = "https://api.openai.com/v1/chat/completions" # Example for OpenAI
    model = "gemini-1.5-flash-latest" # Example for Gemini
    api_key = "" # Example for Gemini (use env var preferably)
    base_url = "https://generativelanguage.googleapis.com/v1beta/models" # Example for Gemini
    triple_extraction_max_tokens = 2000
    triple_extraction_temperature = 0.2
    max_tokens = 2000 # Adjust as needed
    temperature = 0.1 # Lower temperature for structured output


[embeddings]
    model_name = "all-MiniLM-L6-v2"

[chunking]
    chunk_size = 1000
    overlap = 100

[standardization]
    enabled = true
    [standardization.synonyms]
    # Format: "lowercase_variant" = "Canonical Name"
    "schlumberger" = "SLB"
    "slb global technology center" = "SLB"
    "schlumberger information solutions" = "SLB"
    "university of california, berkeley" = "UC Berkeley"
    "reliance" = "Reliance Industries Limited" # Example
    "ril" = "Reliance Industries Limited" # Example
    "ongc" = "ONGC" # Ensure canonical casing if desired
    "gspc" = "GSPC"
    # Add other known aliases here

[inference]
    enabled = true
    max_tokens = 2000

[visualization]
    edge_smooth = false

[caching]
    enabled = true

[nlp]
    COREFERENCE_RESOLUTION_ENABLED = true  # Master switch
    SPACY_MODEL_NAME = "en_core_web_trf"  # Or your chosen spaCy model